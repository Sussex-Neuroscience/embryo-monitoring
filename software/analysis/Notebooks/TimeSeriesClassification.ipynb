{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Learning for Time Series Classification\n",
    "\n",
    "Because in our experimental environment, when a larva hatches, it will swim around in the experimental environment, which is very likely to interfere with the observation of other larvae. In this case, when a hatched larva covers a hatching larva, the camera records a false set of motion amplitudes. But we can observe that the time series of the incubation process has a similar trend curve, so we can learn and classify the trend of the time series through the machine learning method MLP (Multilayer Perceptron).\n",
    "\n",
    "Our MLP model imitates the method of [(Wang, Yan and Oates, 2016)](https://arxiv.org/abs/1611.06455) by building a neural network with three hidden layers. Each layer has the same parameters, but each layer in turn has 0.1 more dropout operations than the previous layer (0.1 for the first layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.random.seed(813306)\n",
    "\n",
    "\n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "    Y = data[:, 0]\n",
    "    X = data[:, 1:]\n",
    "    return X, Y\n",
    "nb_epochs = 500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filePath = \"..//..//data//\"\n",
    "flist = [\"Raw1\"]\n",
    "for each in flist:\n",
    "    fname = each\n",
    "    x_train, y_train = readucr(filePath + fname + '_TRAIN')\n",
    "    x_test, y_test = readucr(filePath + fname + '_TEST')\n",
    "    nb_classes = len(np.unique(y_test))\n",
    "    y_train = (y_train - y_train.min()) / (y_train.max() - y_train.min()) * (nb_classes - 1)\n",
    "    y_test = (y_test - y_test.min()) / (y_test.max() - y_test.min()) * (nb_classes - 1)\n",
    "    batch_size = min(x_train.shape[0] / 10, 16)\n",
    "\n",
    "    Y_train = keras.utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = keras.utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean) / (x_train_std)\n",
    "\n",
    "    # x_test_min = np.min(x_test, axis = 1, keepdims=1)\n",
    "    # x_test_max = np.max(x_test, axis = 1, keepdims=1)\n",
    "    x_test = (x_test - x_train_mean) / (x_train_std)\n",
    "\n",
    "    #x_train = x_train.reshape(x_train.shape + (1,))\n",
    "    #x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "    x = keras.layers.Input(x_train.shape[1:])\n",
    "    y = keras.layers.Dropout(0.1)(x)\n",
    "    y = keras.layers.Dense(500, activation='relu')(y)\n",
    "\n",
    "    y = keras.layers.Dropout(0.2)(y)\n",
    "    y = keras.layers.Dense(500, activation='relu')(y)\n",
    "\n",
    "    y = keras.layers.Dropout(0.3)(y)\n",
    "    y = keras.layers.Dense(500, activation='relu')(y)\n",
    "\n",
    "    y = keras.layers.Dropout(0.4)(y)\n",
    "    out = keras.layers.Dense(nb_classes, activation='softmax')(y)\n",
    "\n",
    "    model = keras.models.Model(inputs=x, outputs=out)\n",
    "\n",
    "    optimizer = keras.optimizers.Adadelta()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                                                  patience=200, min_lr=0.1)\n",
    "\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n",
    "                     verbose=1, validation_data=(x_test, Y_test),\n",
    "                     #callbacks = [TestCallback((x_train, Y_train)), reduce_lr, keras.callbacks.TensorBoard(log_dir='./log'+fname, histogram_freq=1)])\n",
    "                     callbacks=[reduce_lr])\n",
    "\n",
    "    #Print the testing results which has the lowest training loss.\n",
    "    log = pd.DataFrame(hist.history)\n",
    "    print(log.loc[log['loss'].idxmin]['loss'], log.loc[log['loss'].idxmin]['val_acc'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[1] Wang, Z., Yan, W. and Oates, T. (2016) ‘Time series classification from scratch with deep neural networks: a strong baseline’. arXiv. Available at: https://doi.org/10.48550/arXiv.1611.06455.\n",
    "\n",
    "[2] Karim, F. et al. (2018) ‘Lstm fully convolutional networks for time series classification’, IEEE Access, 6, pp. 1662–1669. Available at: https://doi.org/10.1109/ACCESS.2017.2779939."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}